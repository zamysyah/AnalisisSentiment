# -*- coding: utf-8 -*-
"""AnalysisSentiment_LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KepbO0TOKpHRpaMTcPXV5vTFESC000ma

1. Import Library
Memuat library penting untuk:

Preprocessing teks (re, string, nltk)

Model dan evaluasi (sklearn)

Penyimpanan model (joblib)
"""

# 1. Import Library
import pandas as pd
import numpy as np
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import joblib

# Download stopwords jika belum
nltk.download('stopwords')

"""2. Load Dataset
Membaca IMDB Dataset.csv ke dalam DataFrame df.
"""

import csv # Import the csv module

# Try to fix the parsing error by specifying error handling and quoting behavior
df = pd.read_csv('IMDB_Dataset.csv', on_bad_lines='skip', quoting=csv.QUOTE_NONNUMERIC)
# on_bad_lines='skip': Skips lines with parsing errors.
# quoting=csv.QUOTE_NONNUMERIC: Ensures all strings are quoted, which might resolve unclosed quotes issue.

df.dropna(inplace=True)

# Tampilkan 5 data teratas
print(df.head())

# Cek distribusi label
print(df['sentiment'].value_counts())

"""**3. Preprocessing Teks**"""

# Fungsi preprocessing teks

stop_words = set(stopwords.words('english'))
stemmer = SnowballStemmer("english")

def clean_text(text):
    if pd.isnull(text):
        return ""
    text = text.lower()  # lowercase
    text = re.sub('<.*?>', '', text)  # hapus HTML
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)  # hapus tanda baca
    text = re.sub('\w*\d\w*', '', text)  # hapus angka
    text = re.sub('\s+', ' ', text).strip()  # hapus spasi berlebih
    stop_words = set(stopwords.words('english'))
    text = ' '.join(word for word in text.split() if word not in stop_words)  # hapus stopwords
    return text

# Terapkan preprocessing
df['clean_review'] = df['review'].apply(clean_text)

"""**‚öôÔ∏è 4. Label Encoding**"""

df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})

"""**üß† 5. TF-IDF Vectorization**"""

# Inisialisasi TF-IDF
tfidf = TfidfVectorizer(max_features=5000)

# Fit dan transformasi pada teks bersih
X = tfidf.fit_transform(df['clean_review'])

# Target variabel
y = df['label']

"""**üß™ 6. Train-Test Split**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""** 7. Training Model (Logistic Regression)**"""

model = LogisticRegression()
model.fit(X_train, y_train)

"""**‚úÖ 8. Evaluasi Model**"""

y_pred = model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""**üíæ 9. Simpan Model**"""

joblib.dump(model, 'sentiment_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

import joblib

# Load kembali model dan vectorizer
model = joblib.load('sentiment_model.pkl')
tfidf = joblib.load('tfidf_vectorizer.pkl')

# Function untuk bersihkan dan prediksi review baru
def predict_sentiment(review):
    # Preprocessing
    review = review.lower()
    review = re.sub('<.*?>', '', review)
    review = re.sub('[%s]' % re.escape(string.punctuation), '', review)
    review = re.sub('\w*\d\w*', '', review)
    review = ' '.join([word for word in review.split() if word not in stopwords.words('english')])

    # TF-IDF transform
    review_vector = tfidf.transform([review])

    # Prediksi
    prediction = model.predict(review_vector)

    # Output label
    return 'Positive üòä' if prediction[0] == 1 else 'Negative üòû'

sample_review = "The movie was boring"
print(predict_sentiment(sample_review))

sample_review = "The movie is unrealistis, to good"
print(predict_sentiment(sample_review))

# Load model dan vectorizer
import joblib
import re
import string
from nltk.corpus import stopwords

model = joblib.load('sentiment_model.pkl')
tfidf = joblib.load('tfidf_vectorizer.pkl')

def clean_review(text):
    text = text.lower()
    text = re.sub('<.*?>', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

def predict_sentiment(review):
    cleaned = clean_review(review)
    vec = tfidf.transform([cleaned])
    result = model.predict(vec)
    return "Positive üòä" if result[0] == 1 else "Negative üòû"

# Jalankan CLI
while True:
    review_input = input("Masukkan review film (atau ketik 'exit' untuk keluar): ")
    if review_input.lower() == 'exit':
        print("Terima kasih! üôè")
        break
    prediction = predict_sentiment(review_input)
    print(f"Hasil prediksi sentimen: {prediction}")

!pip install streamlit

import streamlit as st

# Load model dan vectorizer
model = joblib.load('sentiment_model.pkl')
tfidf = joblib.load('tfidf_vectorizer.pkl')

def clean_text(text):
    text = text.lower()
    text = re.sub('<.*?>', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

def predict_sentiment(text):
    cleaned = clean_text(text)
    vec = tfidf.transform([cleaned])
    result = model.predict(vec)
    return "Positive üòä" if result[0] == 1 else "Negative üòû"

# Streamlit UI
st.set_page_config(page_title="Sentiment Analyzer", layout="centered")
st.title("üé¨ Movie Review Sentiment Analyzer")
st.write("Masukkan review film dan lihat apakah sentimennya positif atau negatif!")

user_input = st.text_area("Tulis review di sini:")

if st.button("Prediksi Sentimen"):
    if user_input:
        result = predict_sentiment(user_input)
        st.success(f"Hasil Prediksi: {result}")
    else:
        st.warning("Tolong masukkan review terlebih dahulu.")

code = '''
import streamlit as st
import joblib
import re
import string
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')

# Load model dan vectorizer
model = joblib.load('sentiment_model.pkl')
tfidf = joblib.load('tfidf_vectorizer.pkl')

def clean_text(text):
    text = text.lower()
    text = re.sub('<.*?>', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\\w*\\d\\w*', '', text)
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

def predict_sentiment(text):
    cleaned = clean_text(text)
    vec = tfidf.transform([cleaned])
    result = model.predict(vec)
    return "Positive üòä" if result[0] == 1 else "Negative üòû"

# Streamlit UI
st.set_page_config(page_title="Sentiment Analyzer", layout="centered")
st.title("üé¨ Movie Review Sentiment Analyzer")
st.write("Masukkan review film dan lihat apakah sentimennya positif atau negatif!")

user_input = st.text_area("Tulis review di sini:")

if st.button("Prediksi Sentimen"):
    if user_input:
        result = predict_sentiment(user_input)
        st.success(f"Hasil Prediksi: {result}")
    else:
        st.warning("Tolong masukkan review terlebih dahulu.")
'''

with open('app.py', 'w') as f:
    f.write(code)

!pip install pyngrok

!ngrok config add-authtoken Your Auth Token

from pyngrok import ngrok

# Jalankan Streamlit di background
!streamlit run app.py &>/content/logs.txt &

# Koneksikan ngrok dengan benar
public_url = ngrok.connect(addr="8502", proto="http")
print("Akses aplikasi kamu di URL:", public_url)